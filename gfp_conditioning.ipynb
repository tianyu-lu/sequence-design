{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras import layers, initializers\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from seqtools import SequenceTools as ST\n",
    "from gfp_gp import SequenceGP\n",
    "from util import AA, AA_IDX\n",
    "from util import build_vae\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gan import WGAN\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from util import one_hot_encode_aa, partition_data, get_balaji_predictions, get_samples\n",
    "from util import convert_idx_array_to_aas, build_pred_vae_model, get_experimental_X_y\n",
    "from util import get_gfp_X_y_aa\n",
    "from losses import neg_log_likelihood\n",
    "import json\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(M):\n",
    "    x = Input(shape=(M, 20,))\n",
    "    y = Flatten()(x)\n",
    "    y = Dense(50, activation='elu')(y)\n",
    "    y = Dense(2)(y)\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model\n",
    "\n",
    "def evaluate_ground_truth(X_aa, ground_truth, save_file=None):\n",
    "    y_gt = ground_truth.predict(X_aa, print_every=100000)[:, 0]\n",
    "    if save_file is not None:\n",
    "        np.save(save_file, y_gt)\n",
    "        \n",
    "def train_and_save_oracles(X_train, y_train, n=10, suffix='', batch_size=100):\n",
    "    for i in range(n):\n",
    "        model = build_model(X_train.shape[1])\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=neg_log_likelihood,\n",
    "                      )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', \n",
    "                                   min_delta=0, \n",
    "                                   patience=5, \n",
    "                                   verbose=1)\n",
    "\n",
    "        model.fit(X_train, y_train, \n",
    "                  epochs=100, \n",
    "                  batch_size=batch_size, \n",
    "                  validation_split=0.1, \n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=2)\n",
    "        model.save(\"models/oracle_%i%s.h5\" % (i, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_ml_opt(X_train, oracles, ground_truth, vae_0, weights_type='dbas',\n",
    "        LD=20, iters=20, samples=500, homoscedastic=False, homo_y_var=0.1,\n",
    "        quantile=0.95, verbose=False, alpha=1, train_gt_evals=None,\n",
    "        cutoff=1e-6, it_epochs=10, enc1_units=50):\n",
    "    \n",
    "    assert weights_type in ['cbas', 'dbas','rwr', 'cem-pi', 'fbvae']\n",
    "    L = X_train.shape[1]\n",
    "    vae = build_vae(latent_dim=LD,\n",
    "                    n_tokens=20, seq_length=L,\n",
    "                    enc1_units=enc1_units)\n",
    "\n",
    "    traj = np.zeros((iters, 7))\n",
    "    oracle_samples = np.zeros((iters, samples))\n",
    "    gt_samples = np.zeros((iters, samples))\n",
    "    oracle_max_seq = None\n",
    "    oracle_max = -np.inf\n",
    "    gt_of_oracle_max = -np.inf\n",
    "    y_star = -np.inf\n",
    "    \n",
    "    \n",
    "    # FOR REVIEW:\n",
    "    all_seqs = pd.DataFrame(0, index=range(int((iters-1)*samples)), columns=['seq', 'val'])\n",
    "    l_ = 0\n",
    "    \n",
    "    \n",
    "    for t in range(iters):\n",
    "        ### Take Samples ###\n",
    "        zt = np.random.randn(samples, LD)\n",
    "        if t > 0:\n",
    "            Xt_p = vae.decoder_.predict(zt)\n",
    "            Xt = get_samples(Xt_p)\n",
    "        else:\n",
    "            Xt = X_train\n",
    "        \n",
    "        ### Evaluate ground truth and oracle ###\n",
    "        yt, yt_var = get_balaji_predictions(oracles, Xt)\n",
    "        if homoscedastic:\n",
    "            yt_var = np.ones_like(yt) * homo_y_var\n",
    "        Xt_aa = np.argmax(Xt, axis=-1)\n",
    "        if t == 0 and train_gt_evals is not None:\n",
    "            yt_gt = train_gt_evals\n",
    "        else:\n",
    "            yt_gt = ground_truth.predict(Xt_aa, print_every=1000000)[:, 0]\n",
    "        \n",
    "        ### Calculate weights for different schemes ###\n",
    "        if t > 0:\n",
    "            if weights_type == 'cbas': \n",
    "                log_pxt = np.sum(np.log(Xt_p) * Xt, axis=(1, 2))\n",
    "                X0_p = vae_0.decoder_.predict(zt)\n",
    "                log_px0 = np.sum(np.log(X0_p) * Xt, axis=(1, 2))\n",
    "                w1 = np.exp(log_px0-log_pxt)\n",
    "                y_star_1 = np.percentile(yt, quantile*100)\n",
    "                if y_star_1 > y_star:\n",
    "                    y_star = y_star_1\n",
    "                w2= scipy.stats.norm.sf(y_star, loc=yt, scale=np.sqrt(yt_var))\n",
    "                weights = w1*w2 \n",
    "            elif weights_type == 'cem-pi':\n",
    "                pi = scipy.stats.norm.sf(max_train_gt, loc=yt, scale=np.sqrt(yt_var))\n",
    "                pi_thresh = np.percentile(pi, quantile*100)\n",
    "                weights = (pi > pi_thresh).astype(int)\n",
    "            elif weights_type == 'dbas':\n",
    "                y_star_1 = np.percentile(yt, quantile*100)\n",
    "                if y_star_1 > y_star:\n",
    "                    y_star = y_star_1\n",
    "                weights = scipy.stats.norm.sf(y_star, loc=yt, scale=np.sqrt(yt_var))\n",
    "            elif weights_type == 'rwr':\n",
    "                weights = np.exp(alpha*yt)\n",
    "                weights /= np.sum(weights)\n",
    "        else:\n",
    "            weights = np.ones(yt.shape[0])\n",
    "            max_train_gt = np.max(yt_gt)\n",
    "            \n",
    "        yt_max_idx = np.argmax(yt)\n",
    "        yt_max = yt[yt_max_idx]\n",
    "        if yt_max > oracle_max:\n",
    "            oracle_max = yt_max\n",
    "            try:\n",
    "                oracle_max_seq = convert_idx_array_to_aas(Xt_aa[yt_max_idx-1:yt_max_idx])[0]\n",
    "            except IndexError:\n",
    "                print(Xt_aa[yt_max_idx-1:yt_max_idx])\n",
    "            gt_of_oracle_max = yt_gt[yt_max_idx]\n",
    "        \n",
    "        ### Record and print results ##\n",
    "        if t == 0:\n",
    "            rand_idx = np.random.randint(0, len(yt), samples)\n",
    "            oracle_samples[t, :] = yt[rand_idx]\n",
    "            gt_samples[t, :] = yt_gt[rand_idx]\n",
    "        if t > 0:\n",
    "            oracle_samples[t, :] = yt\n",
    "            gt_samples[t, :] = yt_gt\n",
    "        \n",
    "        traj[t, 0] = np.max(yt_gt)\n",
    "        traj[t, 1] = np.mean(yt_gt)\n",
    "        traj[t, 2] = np.std(yt_gt)\n",
    "        traj[t, 3] = np.max(yt)\n",
    "        traj[t, 4] = np.mean(yt)\n",
    "        traj[t, 5] = np.std(yt)\n",
    "        traj[t, 6] = np.mean(yt_var)\n",
    "        \n",
    "        if verbose:\n",
    "            print(weights_type.upper(), t, traj[t, 0], color.BOLD + str(traj[t, 1]) + color.END, \n",
    "                  traj[t, 2], traj[t, 3], color.BOLD + str(traj[t, 4]) + color.END, traj[t, 5], traj[t, 6])\n",
    "        \n",
    "        ### Train model ###\n",
    "        if t == 0:\n",
    "            vae.encoder_.set_weights(vae_0.encoder_.get_weights())\n",
    "            vae.decoder_.set_weights(vae_0.decoder_.get_weights())\n",
    "            vae.vae_.set_weights(vae_0.vae_.get_weights())\n",
    "        else:\n",
    "            cutoff_idx = np.where(weights < cutoff)\n",
    "            Xt = np.delete(Xt, cutoff_idx, axis=0)\n",
    "            yt = np.delete(yt, cutoff_idx, axis=0)\n",
    "            weights = np.delete(weights, cutoff_idx, axis=0)\n",
    "            vae.fit([Xt], [Xt, np.zeros(Xt.shape[0])],\n",
    "                  epochs=it_epochs,\n",
    "                  batch_size=10,\n",
    "                  shuffle=False,\n",
    "                  sample_weight=[weights, weights],\n",
    "                  verbose=0)\n",
    "    \n",
    "    max_dict = {'oracle_max' : oracle_max, \n",
    "                'oracle_max_seq': oracle_max_seq, \n",
    "                'gt_of_oracle_max': gt_of_oracle_max}\n",
    "    return traj, oracle_samples, gt_samples, max_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_opt(X_train, oracles, ground_truth, vae_0, weights_type='fbvae',\n",
    "        LD=20, iters=20, samples=500, \n",
    "        quantile=0.8, verbose=False, train_gt_evals=None,\n",
    "        it_epochs=10, enc1_units=50):\n",
    "    \n",
    "    assert weights_type in ['fbvae']\n",
    "    L = X_train.shape[1]\n",
    "    vae = build_vae(latent_dim=LD,\n",
    "                    n_tokens=20, seq_length=L,\n",
    "                    enc1_units=enc1_units)\n",
    "\n",
    "    traj = np.zeros((iters, 7))\n",
    "    oracle_samples = np.zeros((iters, samples))\n",
    "    gt_samples = np.zeros((iters, samples))\n",
    "    oracle_max_seq = None\n",
    "    oracle_max = -np.inf\n",
    "    gt_of_oracle_max = -np.inf\n",
    "    y_star = - np.inf\n",
    "    for t in range(iters):\n",
    "        ### Take Samples and evaluate ground truth and oracle ##\n",
    "        zt = np.random.randn(samples, LD)\n",
    "        if t > 0:\n",
    "            Xt_sample_p = vae.decoder_.predict(zt)\n",
    "            Xt_sample = get_samples(Xt_sample_p)\n",
    "            yt_sample, _ = get_balaji_predictions(oracles, Xt_sample)\n",
    "            Xt_aa_sample = np.argmax(Xt_sample, axis=-1)\n",
    "            yt_gt_sample = ground_truth.predict(Xt_aa_sample, print_every=1000000)[:, 0]\n",
    "        else:\n",
    "            Xt = X_train\n",
    "            yt, _ = get_balaji_predictions(oracles, Xt)\n",
    "            Xt_aa = np.argmax(Xt, axis=-1)\n",
    "            fb_thresh = np.percentile(yt, quantile*100)\n",
    "            if train_gt_evals is not None:\n",
    "                yt_gt = train_gt_evals\n",
    "            else:\n",
    "                yt_gt = ground_truth.predict(Xt_aa, print_every=1000000)[:, 0]\n",
    "        \n",
    "        ### Calculate threshold ###\n",
    "        if t > 0:\n",
    "            threshold_idx = np.where(yt_sample >= fb_thresh)[0]\n",
    "            n_top = len(threshold_idx)\n",
    "            sample_arrs = [Xt_sample, yt_sample, yt_gt_sample, Xt_aa_sample]\n",
    "            full_arrs = [Xt, yt, yt_gt, Xt_aa]\n",
    "            \n",
    "            for l in range(len(full_arrs)):\n",
    "                sample_arr = sample_arrs[l]\n",
    "                full_arr = full_arrs[l]\n",
    "                sample_top = sample_arr[threshold_idx]\n",
    "                full_arr = np.concatenate([sample_top, full_arr])\n",
    "                full_arr = np.delete(full_arr, range(full_arr.shape[0]-n_top, full_arr.shape[0]), axis=0)\n",
    "                full_arrs[l] = full_arr\n",
    "            Xt, yt, yt_gt, Xt_aa = full_arrs\n",
    "        yt_max_idx = np.argmax(yt)\n",
    "        yt_max = yt[yt_max_idx]\n",
    "        if yt_max > oracle_max:\n",
    "            oracle_max = yt_max\n",
    "            try:\n",
    "                oracle_max_seq = convert_idx_array_to_aas(Xt_aa[yt_max_idx-1:yt_max_idx])[0]\n",
    "            except IndexError:\n",
    "                print(Xt_aa[yt_max_idx-1:yt_max_idx])\n",
    "            gt_of_oracle_max = yt_gt[yt_max_idx]\n",
    "        \n",
    "        ### Record and print results ##\n",
    "\n",
    "        rand_idx = np.random.randint(0, len(yt), samples)\n",
    "        oracle_samples[t, :] = yt[rand_idx]\n",
    "        gt_samples[t, :] = yt_gt[rand_idx]\n",
    "\n",
    "        traj[t, 0] = np.max(yt_gt)\n",
    "        traj[t, 1] = np.mean(yt_gt)\n",
    "        traj[t, 2] = np.std(yt_gt)\n",
    "        traj[t, 3] = np.max(yt)\n",
    "        traj[t, 4] = np.mean(yt)\n",
    "        traj[t, 5] = np.std(yt)\n",
    "        if t > 0:\n",
    "            traj[t, 6] = n_top\n",
    "        else:\n",
    "            traj[t, 6] = 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(weights_type.upper(), t, traj[t, 0], color.BOLD + str(traj[t, 1]) + color.END, \n",
    "                  traj[t, 2], traj[t, 3], color.BOLD + str(traj[t, 4]) + color.END, traj[t, 5], traj[t, 6])\n",
    "        \n",
    "        ### Train model ###\n",
    "        if t == 0:\n",
    "            vae.encoder_.set_weights(vae_0.encoder_.get_weights())\n",
    "            vae.decoder_.set_weights(vae_0.decoder_.get_weights())\n",
    "            vae.vae_.set_weights(vae_0.vae_.get_weights())\n",
    "        else:\n",
    "        \n",
    "            vae.fit([Xt], [Xt, np.zeros(Xt.shape[0])],\n",
    "                  epochs=1,\n",
    "                  batch_size=10,\n",
    "                  shuffle=False,\n",
    "                  verbose=0)\n",
    "            \n",
    "    max_dict = {'oracle_max' : oracle_max, \n",
    "                'oracle_max_seq': oracle_max_seq, \n",
    "                'gt_of_oracle_max': gt_of_oracle_max}\n",
    "    return traj, oracle_samples, gt_samples, max_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experimental_oracles():\n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    i = 1\n",
    "    num_models = [1, 5, 20]\n",
    "    for i in range(len(num_models)):\n",
    "        RANDOM_STATE = i+1\n",
    "        nm = num_models[i]\n",
    "        X_train, y_train, _  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "        suffix = '_%s_%i_%i' % (train_size_str, nm, RANDOM_STATE)\n",
    "        train_and_save_oracles(X_train, y_train, batch_size=10, n=nm, suffix=suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experimental_vaes():\n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    suffix = '_%s' % train_size_str\n",
    "    for i in [0, 2]:\n",
    "        RANDOM_STATE = i + 1\n",
    "        X_train, _, _  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "        vae_0 = build_vae(latent_dim=20,\n",
    "                  n_tokens=20, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "        vae_0.fit([X_train], [X_train, np.zeros(X_train.shape[0])],\n",
    "                  epochs=100,\n",
    "                  batch_size=10,\n",
    "                  verbose=2)\n",
    "        vae_0.encoder_.save_weights(\"models/vae_0_encoder_weights%s_%i.h5\"% (suffix, RANDOM_STATE))\n",
    "        vae_0.decoder_.save_weights(\"models/vae_0_decoder_weights%s_%i.h5\"% (suffix, RANDOM_STATE))\n",
    "        vae_0.vae_.save_weights(\"models/vae_0_vae_weights%s_%i.h5\"% (suffix, RANDOM_STATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experimental_weighted_ml(it, repeats=3):\n",
    "    \n",
    "    assert it in [0, 1, 2]\n",
    "    \n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    num_models = [1, 5, 20][it]\n",
    "    RANDOM_STATE = it + 1\n",
    "    \n",
    "    X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "    \n",
    "    vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "    oracle_suffix = '_%s_%i_%i' % (train_size_str, num_models, RANDOM_STATE)\n",
    "    \n",
    "    vae_0 = build_vae(latent_dim=20,\n",
    "                  n_tokens=20, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "\n",
    "    vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "    vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "    vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "    \n",
    "    ground_truth = SequenceGP(load=True, load_prefix=\"data/gfp_gp\")\n",
    "    \n",
    "    loss = neg_log_likelihood\n",
    "    get_custom_objects().update({\"neg_log_likelihood\": loss})\n",
    "    oracles = [load_model(\"models/oracle_%i%s.h5\" % (i, oracle_suffix)) for i in range(num_models)]\n",
    "    \n",
    "    test_kwargs = [\n",
    "                   {'weights_type':'cbas', 'quantile': 1},\n",
    "                   {'weights_type':'rwr', 'alpha': 20},\n",
    "                   {'weights_type':'dbas', 'quantile': 0.95},\n",
    "                   {'weights_type':'cem-pi', 'quantile': 0.8},\n",
    "                   {'weights_type': 'fbvae', 'quantile': 0.8}\n",
    "    ]\n",
    "    \n",
    "    base_kwargs = {\n",
    "        'homoscedastic': False,\n",
    "        'homo_y_var': 0.01,\n",
    "        'train_gt_evals':gt_train,\n",
    "        'samples':100,\n",
    "        'cutoff':1e-6,\n",
    "        'it_epochs':10,\n",
    "        'verbose':True,\n",
    "        'LD': 20,\n",
    "        'enc1_units':50,\n",
    "        'iters': 50\n",
    "    }\n",
    "    \n",
    "    if num_models==1:\n",
    "        base_kwargs['homoscedastic'] = True\n",
    "        base_kwargs['homo_y_var'] = np.mean((get_balaji_predictions(oracles, X_train)[0] - y_train)**2)\n",
    "    \n",
    "    for k in range(repeats):\n",
    "        for j in range(len(test_kwargs)):\n",
    "            test_name = test_kwargs[j]['weights_type']\n",
    "            suffix = \"_%s_%i_%i\" % (train_size_str, RANDOM_STATE, k)\n",
    "            if test_name == 'fbvae':\n",
    "                if base_kwargs['iters'] > 100:\n",
    "                    suffix += '_long'\n",
    "            \n",
    "                print(suffix)\n",
    "                kwargs = {}\n",
    "                kwargs.update(test_kwargs[j])\n",
    "                kwargs.update(base_kwargs)\n",
    "                [kwargs.pop(k) for k in ['homoscedastic', 'homo_y_var', 'cutoff', 'it_epochs']]\n",
    "                test_traj, test_oracle_samples, test_gt_samples, test_max = fb_opt(np.copy(X_train), oracles, ground_truth, vae_0, **kwargs)\n",
    "            else:\n",
    "                if base_kwargs['iters'] > 100:\n",
    "                    suffix += '_long'\n",
    "                kwargs = {}\n",
    "                kwargs.update(test_kwargs[j])\n",
    "                kwargs.update(base_kwargs)\n",
    "                test_traj, test_oracle_samples, test_gt_samples, test_max = weighted_ml_opt(np.copy(X_train), oracles, ground_truth, vae_0, **kwargs)\n",
    "            np.save('results/%s_traj%s.npy' %(test_name, suffix), test_traj)\n",
    "            np.save('results/%s_oracle_samples%s.npy' % (test_name, suffix), test_oracle_samples)\n",
    "            np.save('results/%s_gt_samples%s.npy'%(test_name, suffix), test_gt_samples )\n",
    "\n",
    "            with open('results/%s_max%s.json'% (test_name, suffix), 'w') as outfile:\n",
    "                json.dump(test_max, outfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cbas_q(qs = [0.5, 0.75, 0.95, 1]):\n",
    "    it = 0\n",
    "    \n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    num_models = [1, 5, 20][it]\n",
    "    RANDOM_STATE = it + 1\n",
    "    \n",
    "    X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "    \n",
    "    vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "    oracle_suffix = '_%s_%i_%i' % (train_size_str, num_models, RANDOM_STATE)\n",
    "    \n",
    "    vae_0 = build_vae(latent_dim=20,\n",
    "                  n_tokens=20, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "\n",
    "    vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "    vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "    vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "    \n",
    "    ground_truth = SequenceGP(load=True, load_prefix=\"data/gfp_gp\")\n",
    "    \n",
    "    loss = neg_log_likelihood\n",
    "    get_custom_objects().update({\"neg_log_likelihood\": loss})\n",
    "    oracles = [load_model(\"models/oracle_%i%s.h5\" % (i, oracle_suffix)) for i in range(num_models)]\n",
    "    \n",
    "    test_kwargs = [ {'weights_type':'cbas', 'quantile': q} for q in qs]\n",
    "    \n",
    "    base_kwargs = {\n",
    "        'homoscedastic': False,\n",
    "        'homo_y_var': 0.01,\n",
    "        'train_gt_evals':gt_train,\n",
    "        'samples':100,\n",
    "        'cutoff':1e-6,\n",
    "        'it_epochs':10,\n",
    "        'verbose':True,\n",
    "        'LD': 20,\n",
    "        'enc1_units':50,\n",
    "        'iters':100\n",
    "    }\n",
    "    \n",
    "    if num_models==1:\n",
    "        base_kwargs['homoscedastic'] = True\n",
    "        base_kwargs['homo_y_var'] = np.mean((get_balaji_predictions(oracles, X_train)[0] - y_train)**2)\n",
    "    \n",
    "    for j in range(len(test_kwargs)):\n",
    "        test_name = test_kwargs[j]['weights_type']\n",
    "        qj = test_kwargs[j]['quantile']\n",
    "        suffix = \"_qtest_%s_%i_%.2f\" % (train_size_str, RANDOM_STATE, qj)\n",
    "        print(suffix)\n",
    "        kwargs = {}\n",
    "        kwargs.update(test_kwargs[j])\n",
    "        kwargs.update(base_kwargs)\n",
    "        test_traj, test_oracle_samples, test_gt_samples, test_max = weighted_ml_opt(np.copy(X_train), oracles, ground_truth, vae_0, **kwargs)\n",
    "        \n",
    "        np.save('results/%s_traj%s.npy' %(test_name, suffix), test_traj)\n",
    "        np.save('results/%s_oracle_samples%s.npy' % (test_name, suffix), test_oracle_samples)\n",
    "        np.save('results/%s_gt_samples%s.npy'%(test_name, suffix), test_gt_samples )\n",
    "\n",
    "        with open('results/%s_max%s.json'% (test_name, suffix), 'w') as outfile:\n",
    "            json.dump(test_max, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_5k_1_0\n",
      "CBAS 0 3.1522492716011907 \u001b[1m3.1114846665730953\u001b[0m 0.036913624308314026 3.185194730758667 \u001b[1m3.1188620679855346\u001b[0m 0.02476560246847297 0.0007276618358598761\n",
      "CBAS 1 3.343200333472878 \u001b[1m3.1372669984822266\u001b[0m 0.08542544879038347 3.1836001873016357 \u001b[1m3.1255055832862855\u001b[0m 0.03757263871629514 0.0007276618358598758\n",
      "CBAS 2 3.3200958737603283 \u001b[1m3.206044893124411\u001b[0m 0.06534528851399343 3.1916959285736084 \u001b[1m3.149348895549774\u001b[0m 0.0232937581283955 0.0007276618358598758\n",
      "CBAS 3 3.353829579982408 \u001b[1m3.2272006237767954\u001b[0m 0.06136721276436493 3.1934802532196045 \u001b[1m3.1577219104766847\u001b[0m 0.018347936844734566 0.0007276618358598758\n",
      "CBAS 4 3.4121971446578723 \u001b[1m3.2309365151930445\u001b[0m 0.05999169217094124 3.1904842853546143 \u001b[1m3.1590127110481263\u001b[0m 0.017228692024066457 0.0007276618358598758\n",
      "CBAS 5 3.4121971446578723 \u001b[1m3.2396768796166953\u001b[0m 0.05992245477338031 3.2004764080047607 \u001b[1m3.161492450237274\u001b[0m 0.018917833816742342 0.0007276618358598758\n",
      "CBAS 6 3.3283396546772055 \u001b[1m3.227905086036717\u001b[0m 0.052684837922109755 3.2099897861480713 \u001b[1m3.1583284902572633\u001b[0m 0.019288513551856474 0.0007276618358598758\n",
      "CBAS 7 3.3285615508201243 \u001b[1m3.228364431269788\u001b[0m 0.06403358838814191 3.1958811283111572 \u001b[1m3.1592081451416014\u001b[0m 0.017946500869456844 0.0007276618358598758\n",
      "CBAS 8 3.3717502301429203 \u001b[1m3.2268663815997254\u001b[0m 0.06895005913085649 3.1944580078125 \u001b[1m3.1553001689910887\u001b[0m 0.02438936731656345 0.0007276618358598758\n",
      "CBAS 9 3.3577081591314757 \u001b[1m3.2533101420286754\u001b[0m 0.04900839459074888 3.2010200023651123 \u001b[1m3.1621919751167296\u001b[0m 0.012451834900509945 0.0007276618358598758\n",
      "CBAS 10 3.443391350418846 \u001b[1m3.24661938583612\u001b[0m 0.06112221894574912 3.2119038105010986 \u001b[1m3.162676808834076\u001b[0m 0.017728928397528345 0.0007276618358598758\n",
      "CBAS 11 3.385031508599072 \u001b[1m3.2318651951660424\u001b[0m 0.06260322617543815 3.2328598499298096 \u001b[1m3.1587326455116274\u001b[0m 0.02219894591706268 0.0007276618358598758\n",
      "CBAS 12 3.4121971446578723 \u001b[1m3.237694489866689\u001b[0m 0.054803862697337405 3.2027204036712646 \u001b[1m3.1619212794303895\u001b[0m 0.014814896244705634 0.0007276618358598758\n",
      "CBAS 13 3.4121971446578723 \u001b[1m3.2274666074484832\u001b[0m 0.06358758107231073 3.1938817501068115 \u001b[1m3.1582717227935793\u001b[0m 0.018659695635668005 0.0007276618358598758\n",
      "CBAS 14 3.4121971446578723 \u001b[1m3.2444121673398554\u001b[0m 0.057721944493132595 3.1992132663726807 \u001b[1m3.1643254733085633\u001b[0m 0.016450316071884193 0.0007276618358598758\n",
      "CBAS 15 3.4121971446578723 \u001b[1m3.2397561170198372\u001b[0m 0.06203857579274885 3.2233827114105225 \u001b[1m3.1612169218063353\u001b[0m 0.016822732247110294 0.0007276618358598758\n",
      "CBAS 16 3.3881188898699968 \u001b[1m3.2500044847484486\u001b[0m 0.04471918921863315 3.2223005294799805 \u001b[1m3.165328574180603\u001b[0m 0.013440619818268468 0.0007276618358598758\n",
      "CBAS 17 3.3781864918825333 \u001b[1m3.24691396638094\u001b[0m 0.04419935325373387 3.2115609645843506 \u001b[1m3.1640165877342223\u001b[0m 0.013753194608005635 0.0007276618358598758\n",
      "CBAS 18 3.4379157597339214 \u001b[1m3.24938931708118\u001b[0m 0.05609777291126229 3.2142090797424316 \u001b[1m3.1634296488761904\u001b[0m 0.01382922371495457 0.0007276618358598758\n",
      "CBAS 19 3.343119568547504 \u001b[1m3.2420288928329035\u001b[0m 0.047440166271175775 3.2103002071380615 \u001b[1m3.162458236217499\u001b[0m 0.01529230216633177 0.0007276618358598758\n",
      "CBAS 20 3.377061017682612 \u001b[1m3.2476485334311787\u001b[0m 0.05582012754168704 3.2103002071380615 \u001b[1m3.1647889494895933\u001b[0m 0.018480383857444035 0.0007276618358598758\n",
      "CBAS 21 3.4121971446578723 \u001b[1m3.2704734579149664\u001b[0m 0.04826125223410578 3.2130706310272217 \u001b[1m3.168564805984497\u001b[0m 0.015712247239275647 0.0007276618358598758\n",
      "CBAS 22 3.45689212089329 \u001b[1m3.2654453885102646\u001b[0m 0.05510552519819267 3.199016571044922 \u001b[1m3.168006818294525\u001b[0m 0.012578557364928114 0.0007276618358598758\n",
      "CBAS 23 3.406503351894849 \u001b[1m3.2658554174611383\u001b[0m 0.05415294070215034 3.2218563556671143 \u001b[1m3.1669720363616944\u001b[0m 0.014742989216985724 0.0007276618358598758\n",
      "CBAS 24 3.4121971446578723 \u001b[1m3.271028673183923\u001b[0m 0.050922193270044235 3.1974475383758545 \u001b[1m3.166643273830414\u001b[0m 0.012521967164518616 0.0007276618358598758\n",
      "CBAS 25 3.45689212089329 \u001b[1m3.2689991714633773\u001b[0m 0.05275071568918439 3.2171413898468018 \u001b[1m3.1658988285064695\u001b[0m 0.016115295271415163 0.0007276618358598758\n",
      "CBAS 26 3.3434241865340057 \u001b[1m3.2566815485485954\u001b[0m 0.05121051645520069 3.202911138534546 \u001b[1m3.165556998252869\u001b[0m 0.016721377897554957 0.0007276618358598758\n",
      "CBAS 27 3.4121971446578723 \u001b[1m3.270743521711202\u001b[0m 0.04607155691256505 3.1934258937835693 \u001b[1m3.166972782611847\u001b[0m 0.012438211558509642 0.0007276618358598758\n",
      "CBAS 28 3.4121971446578723 \u001b[1m3.2673416402263813\u001b[0m 0.05686827767755792 3.227677345275879 \u001b[1m3.16722608089447\u001b[0m 0.015915714498996245 0.0007276618358598758\n",
      "CBAS 29 3.3612969450012375 \u001b[1m3.2663599077888574\u001b[0m 0.05040619921161473 3.2014241218566895 \u001b[1m3.1662690925598143\u001b[0m 0.012505857961598789 0.0007276618358598758\n",
      "CBAS 30 3.4146461697600508 \u001b[1m3.270302791836985\u001b[0m 0.0489562249351802 3.2136690616607666 \u001b[1m3.168308970928192\u001b[0m 0.014921965381587004 0.0007276618358598758\n",
      "CBAS 31 3.4121971446578723 \u001b[1m3.26663371208892\u001b[0m 0.047377886316489294 3.223515748977661 \u001b[1m3.1664212274551393\u001b[0m 0.014100011841287997 0.0007276618358598758\n",
      "CBAS 32 3.352704211631048 \u001b[1m3.2682718625817047\u001b[0m 0.04596174058998017 3.205033302307129 \u001b[1m3.1672349166870117\u001b[0m 0.01313683980663015 0.0007276618358598758\n",
      "CBAS 33 3.4121971446578723 \u001b[1m3.2543953951704117\u001b[0m 0.05559864881178323 3.2218434810638428 \u001b[1m3.165801682472229\u001b[0m 0.015554148777783678 0.0007276618358598758\n",
      "CBAS 34 3.352704211631048 \u001b[1m3.2587490965088857\u001b[0m 0.0481020873801073 3.2103002071380615 \u001b[1m3.1663675117492676\u001b[0m 0.015369271804939797 0.0007276618358598758\n",
      "CBAS 35 3.4121971446578723 \u001b[1m3.2666011690704733\u001b[0m 0.051730819751137254 3.213745355606079 \u001b[1m3.1677934193611144\u001b[0m 0.0154378186939111 0.0007276618358598758\n",
      "CBAS 36 3.4121971446578723 \u001b[1m3.2528681802916406\u001b[0m 0.051119735828780204 3.1927216053009033 \u001b[1m3.1647455048561097\u001b[0m 0.011911627729386897 0.0007276618358598758\n",
      "CBAS 37 3.407564316527939 \u001b[1m3.2299503224851214\u001b[0m 0.06432114469668955 3.2244925498962402 \u001b[1m3.1615292167663576\u001b[0m 0.02062199534421602 0.0007276618358598758\n",
      "CBAS 38 3.419627131026383 \u001b[1m3.249063443254897\u001b[0m 0.05686865019588655 3.213594675064087 \u001b[1m3.164238061904907\u001b[0m 0.017993818628596846 0.0007276618358598758\n",
      "CBAS 39 3.4121971446578723 \u001b[1m3.256334285077316\u001b[0m 0.056737854671706694 3.2169549465179443 \u001b[1m3.168398516178131\u001b[0m 0.017780853672228317 0.0007276618358598758\n",
      "CBAS 40 3.377061017682612 \u001b[1m3.2540746012255726\u001b[0m 0.05337408911011319 3.216420888900757 \u001b[1m3.1659015727043154\u001b[0m 0.014344233802122307 0.0007276618358598758\n",
      "CBAS 41 3.3794341259251404 \u001b[1m3.2573295816504824\u001b[0m 0.051356041862421625 3.228654623031616 \u001b[1m3.165920419692993\u001b[0m 0.01783503723115046 0.0007276618358598758\n",
      "CBAS 42 3.443391350418846 \u001b[1m3.251184807381006\u001b[0m 0.05390459809765154 3.2119038105010986 \u001b[1m3.165274407863617\u001b[0m 0.016766702370915113 0.0007276618358598758\n",
      "CBAS 43 3.3600780161810047 \u001b[1m3.2517782947964076\u001b[0m 0.04974500694440225 3.20063853263855 \u001b[1m3.1637757396698\u001b[0m 0.015796997987831594 0.0007276618358598758\n",
      "CBAS 44 3.4143002494442687 \u001b[1m3.25829798386402\u001b[0m 0.05382461240621543 3.2367746829986572 \u001b[1m3.1678535962104797\u001b[0m 0.01846630535348974 0.0007276618358598758\n",
      "CBAS 45 3.4121971446578723 \u001b[1m3.248264947086826\u001b[0m 0.062198588687370826 3.1904842853546143 \u001b[1m3.164189863204956\u001b[0m 0.015200963969714917 0.0007276618358598758\n",
      "CBAS 46 3.334914013441045 \u001b[1m3.2326356075298883\u001b[0m 0.05122986785841039 3.211054801940918 \u001b[1m3.1666254734992982\u001b[0m 0.01600421999345635 0.0007276618358598758\n",
      "CBAS 47 3.3893526355130934 \u001b[1m3.237876966081563\u001b[0m 0.05424999143039942 3.2090046405792236 \u001b[1m3.1649748849868775\u001b[0m 0.01959081873671614 0.0007276618358598758\n",
      "CBAS 48 3.4121971446578723 \u001b[1m3.2513063958147783\u001b[0m 0.04524136691972017 3.207538604736328 \u001b[1m3.1693726658821104\u001b[0m 0.017823866558265917 0.0007276618358598758\n",
      "CBAS 49 3.4121971446578723 \u001b[1m3.252580159821822\u001b[0m 0.04735535098237943 3.215313673019409 \u001b[1m3.1715048122406007\u001b[0m 0.01599215055159312 0.0007276618358598758\n"
     ]
    }
   ],
   "source": [
    "# train_experimental_vaes()\n",
    "# train_experimental_oracles()\n",
    "\n",
    "run_experimental_weighted_ml(0, repeats=1)\n",
    "# test_cbas_xq(qs=[0.5, 0.75, 0.95, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
